<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features.">
  <meta name="keywords" content="DistillNeRF, generalizable NeRF, autonomous driving, foundation model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features</title>
  <style>
    .title {
        white-space: nowrap;
    }
    .highlight {
        color: #85B737; /* Replace with the desired color */
    }
    .section {
        margin-top: -160px; /* Adjust the value as needed */
    }
    .reduce-distance {
            margin-bottom: 10px; /* Specific margin for reducing distance */
        }
    .hero-body {
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center; /* Ensures the text is centered */
        margin-bottom: 60px; /* Adjust this value as needed */
    }
    .hero {
            margin-bottom: -80px; /* Adjust this value as needed */
        }
  </style>
  

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> <span class="highlight">DistillNeRF</span>: Perceiving 3D Scenes from Single-Glance Images by <br> Distilling Neural Fields and Foundation Model Features</h1>
          <div class="is-size-4 publication-authors">
            <span class="univerity-block">
              <strong style="color: #85B737;">NeurIPS 2024</strong>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=HEzCWisAAAAJ&hl=zh-CN">Letian Wang</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://seung-kim.github.io/seungkim/">Seung Wook Kim</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jiawei-yang.github.io/">Jiawei Yang</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=4xwyGM8AAAAJ&hl=en">Cunjun Yu</a><sup>1,4</sup>,</span>
              <span class="author-block">
                <a href="https://www.borisivanovic.com/">Boris Ivanovic</a><sup>1</sup>,</span><br>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=jY_Bcd8AAAAJ&view_op=list_works&sortby=pubdate">Steven Waslander</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://yuewang.xyz/">Yue Wang</a><sup>1,3</sup>,</span>
              <span class="author-block">
                <a href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://web.stanford.edu/~pavone/">Marco Pavone</a><sup>1,5</sup>,</span>
              <span class="author-block">
                <a href="https://karkus.tilda.ws/">Peter Karkus</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>NVIDIA Research,</span>
            <span class="author-block"><sup>2</sup>University of Toronto,</span>
            <span class="author-block"><sup>3</sup>University of Southern California,</span><br>
            <span class="author-block"><sup>4</sup>National University of Singapore,</span>
            <span class="author-block"><sup>5</sup>Stanford University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.12095"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=HRRmYGubTEU" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/NVlabs/distillnerf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Poster Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/distillnerf/distillnerf.github.io/tree/main/static/images/0_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span> -->

              <!-- <span class="link-block">
                <a href="https://github.com/distillnerf/distillnerf.github.io/tree/main/static/images/0_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                  </a>
              </span> -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-bottom: 6rem;">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <img src="./static/images/overview.png">
          
          <p><strong>DistillNeRF is a generalizable model for 3D scene representation, self-supervised by natural sensor streams along with distillation from offline NeRFs and vision foundation models. It supports rendering RGB, depth, and foundation feature images, without test-time per-scene optimization, and enables downstream tasks such as zero-shot 3D semantic occupancy prediction and open-vocabulary text queries.</strong></p>

        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Concurrent Work. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <img src="./static/images/architecture.png">
          
          <p><strong>DistillNeRF model architecture. Left: single-view encoding with two-stage probabilistic depth prediction; Center: multi-view pooling into a sparse hierarchical voxel representation using sparse quantization and convolution; Right: volumetric rendering from sparse hierarchical voxels.</strong></p>

        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="color: #85B737;">Capabilities</h2>
    <p><strong>
      Given single-frame multi-view cameras as input and without test-time per-scene optimization, DistillNeRF can reconstruct RGB images (row 2), estimate depth (row 3), render foundation model features (rows 4, 5) which enables open-vocabulary text queries (rows 6, 7, 8), and predict binary and semantic occupancy in zero shot (rows 9, 10).
    </strong></p>
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/ours.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="color: #85B737;">Novel-View Synthesis</h2>
    <p><strong>
      Given single-frame multi-view cameras as input and without test-time per-scene optimization, we can synthesize novel views.
    </strong></p>
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/novel_view_synthesis_feature.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="color: #85B737;">Generalizability</h2>
    <p><strong>
      Trained on the nuScenes dataset, our model demonstrates strong zero-shot transfer performance on the unseen Waymo NOTR dataset, achieving decent reconstruction quality (row 2). This quality can be further enhanced by applying simple color alterations to account for camera-specific coloring discrepancies (row 3). After fine-tuning (row 4), our model surpasses the offline per-scene optimized EmerNeRF, achieving higher PSNR (29.84 vs. 28.87) and SSIM (0.911 vs. 0.814).
    </strong></p>
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline width="105%">
        <source src="./static/videos/waymo_convert_2scene.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="color: #85B737;">Comparisons</h2>
    <p><strong>
      Our generalizable DistillNerf is on par with SOTA offline per-scene optimized NeRF method (EmerNerf), and significantly outperforms SOTA generalizable methods (UniPAD and SelfOcc).
    </strong></p>
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline width="105%">
        <source src="./static/videos/compare_sota.mp4"
                type="video/mp4">
      </video>
    </div>
  </div>
</section>

    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p><strong>While we distill offline per-scene optimized NeRF (e.g. EmerNeRF) into DistillNeRF, we observe qualitative phenomenon where the student (DistillNeRF) surpassing the teacher (EmerNeRF), as DistillNeRF is more robust to sensor calibration noises and able to generate smoother predictions.</strong></p>
          <img src="./static/images/emernerf_compare.png">
          

        </div>
      </div>
    </div> -->



<section class="hero teaser">
  <div class="container is-max-desktop">
    <p><strong>Comparing to SOTA offline NeRFs: While we distill offline per-scene optimized NeRF (e.g. EmerNeRF) into DistillNeRF, we observe qualitative phenomenon where the student (DistillNeRF) surpassing the teacher (EmerNeRF), as DistillNeRF is more robust to sensor calibration noises and able to generate smoother predictions.</strong></p>

    <div class="hero-body">
      <img src="./static/images/emernerf_compare.png">
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <p><strong>Comparing to SOTA online NeRFs: UniPAD generate blurry RGB reconstruction and shows strong LiDAR-scanning patterns in the rendered depth. SelfOcc generates gray images and inconsistent depth prediction in the high region of the image.</strong></p>
    <div class="hero-body">
      <img src="./static/images/online_nerf_compare.png">
    </div>
  </div>
</section>


    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p><strong>Comparing to other online NeRFs: UniPAD generate blurry RGB reconstruction and shows strong LiDAR-scanning patterns in the rendered depth. SelfOcc generates gray images and inconsistent depth prediction in the high region of the image.</p>
          <img src="./static/images/online_nerf_compare.png">
        </div>
      </div>
    </div> -->
    
  <!-- </div>
</section> -->



<!-- <section class="hero teaser">

<div class="columns is-centered has-text-centered">
  <div class="column is-full-width">
    <div class="content has-text-justified">
      <img src="./static/images/ablation.png">
      
      <p><strong>When removing Distillation from offline NeRFs, we observe downgraded depth prediction accuracy especially in the high region of the image. When removing the parameterized space, the model is only able to predict depth in the limited range.</p>

    </div>
  </div>
</div>
</section> -->


<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="color: #85B737;">Ablation Studies</h2>
    <p><strong>
      <!-- Given single-frame multi-view cameras as input and without test-time per-scene optimization, DistillNeRF can reconstruct RGB images (row 2), estimate depth (row 3), render foundation model features (rows 4, 5) which enables open-vocabulary text queries (rows 6, 7, 8), and predict binary and semantic occupancy in zero shot (rows 9, 10). -->
      <p><strong>When removing Distillation from offline NeRFs, we observe downgraded depth prediction accuracy especially in the high region of the image. When removing the parameterized space, the model is only able to predict depth in the limited range.</p>
    </strong></p>
    <div class="hero-body">
      <img src="./static/images/ablation.png">
      
    </div>
  </div>
</section>




<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="color: #85B737;">Generalizability</h2>
    <p><strong>
      Given single-frame multi-view cameras as input and without test-time per-scene optimization, DistillNeRF can reconstruct RGB images (row 2), estimate depth (row 3), render foundation model features (rows 4, 5) which enables open-vocabulary text queries (rows 6, 7, 8), and predict binary and semantic occupancy in zero shot (rows 9, 10). -->
      <!-- <p><strong>Trained on the nuScenes dataset, our model demonstrates strong zero-shot transfer performance on the unseen Waymo dataset, achieving decent reconstruction quality. This quality can be further enhanced by applying simple color alterations to account for camera-specific color discrepancies. After fine-tuning, our model surpasses the offline per-scene optimized EmerNeRF, achieving higher PSNR (29.84 vs. 28.87) and SSIM (0.9115 vs. 0.814). -->
      <!-- </strong></p> -->
    <!-- <div class="hero-body"> -->
      <!-- <img src="./static/images/ablation.png"> -->
      <!--  -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{wang2024distillnerf,
      title={DistillNeRF: Perceiving 3D Scenes from Single-Glance Images by Distilling Neural Fields and Foundation Model Features}, 
      author={Letian Wang and Seung Wook Kim and Jiawei Yang and Cunjun Yu and Boris Ivanovic and Steven L. Waslander and Yue Wang and Sanja Fidler and Marco Pavone and Peter Karkus},
      year={2024},
      eprint={2406.12095},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
    }</code></pre>
    </div>
  </section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link" href="">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <!-- <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i> -->
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            We thank the authors of <a href="https://nerfies.github.io/">Nerfies</a> that kindly open sourced the
            template of this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- <a href="https://clustrmaps.com/site/1c07g"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=hVB8BKDOSMZDbhrmbQb6Du6W4iXg4fbDmxdI-ewuKdg&cl=ffffff" /></a> -->
<div id="clustrmaps-container" style="display: none;">
  <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=n&d=hVB8BKDOSMZDbhrmbQb6Du6W4iXg4fbDmxdI-ewuKdg'></script>
  <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=70&t=n&d=3pp5wthj_B_tqIuIxSFqmJlNrIjSTCEobtZnxNdSV7M&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script> -->
</div>
</body>
</html>
